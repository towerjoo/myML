## Batch normalization: Accelerating deep network training by reducing internal covariate shift

https://arxiv.org/abs/1502.03167

**Batch Normalization allows us to use much higher learning rates and be less careful about initialization**


### è¦ç‚¹

1. SGD requires careful tuning of model hyperparameters, esp. the learning rate, and initailization
2. saturation problem, gradient vanishing é—®é¢˜é€šå¸¸é€šè¿‡relu, careful initialization, small LRæ¥è§£å†³
3. BNçš„å¥½å¤„
    * normalization step that fixes the means and variances of layer inputs
    *  benefical on gradient flow(gradient vanishingé—®é¢˜ï¼‰, by reducing the dependence of gradients on the scale of parameters or initial values
    * can use higher LR without the risk of divergence
    * regularize the model and reduces the need of Dropout
    * make it possible to use saturating nonlinearities by preventing the network from saturation
4. reduce internal covariate shift
    * covariate shift: change in the distribution of network activations due to the change in network parameters during training
5. ä¸€ä¸ªnonlinearçš„å˜æ¢ï¼šz=g(Wx+b), é‚£ä¹ˆä½¿ç”¨BNåå°±å˜ä¸ºz=g(BN(Wx)) ï¼ˆbä¼šæ¶ˆå»ï¼‰
6. BN enables higher LR, ä½œè€…è¯æ˜äº†å¢å¤§LRä¸å½±å“æ¢¯åº¦ä¼ æ’­
7. BN regularizes the model
8. ç„¶åæ˜¯å„ç§åº”ç”¨åé€Ÿåº¦çš„æå‡å’Œç²¾ç¡®åº¦çš„æå‡


![BN](/images/bn.png)

ä¸Šå›¾å°±æ˜¯normalizationçš„æ“ä½œ

![BN](/images/bn2.png)

ä¸Šå›¾æ˜¯BNçš„ç®—æ³•, ç”¨æ¥ç”Ÿæˆä¸‹ä¸€ä¸ªç½‘ç»œçš„è¾“å…¥çš„å˜æ¢ï¼Œæ‰€è°“çš„BNå˜æ¢ã€‚

![BN](/images/bn3.png)

ä¸Šå›¾ä¸ºBNçš„è®­ç»ƒç®—æ³•ã€‚


### ä¸ªäººç‚¹è¯„

1. è¿™ä¸ªè®ºæ–‡å†™å¾—è¿™æ˜¯å¤ªå¥½äº†ï¼Œç®€æ´è€Œè¯¦ç»†ï¼Œæ²¡æœ‰å«ç³Šå’Œé®æ©
2. BNè¿™æ ·çš„breakthroughçš„ç®—æ³•è¯»èµ·æ¥çœŸå®æ£’ï¼Œç®€å•ä½†æ˜¯å–å¾—çš„æ•ˆæœå¤ªå¥½äº†
3. è¶Šæ˜¯ç®€å•çš„æ–¹æ³•å¾€å¾€è¶Šèƒ½å–å¾—å¥½çš„ç»“æœï¼ˆæ›´å¥½çš„æ•°å­¦æ”¯æŒï¼‰ï¼Œæ›´å¹¿çš„åº”ç”¨åœºæ™¯ç­‰ï¼Œä½†æ˜¯æå‡ºçš„äººæ˜¯éœ€è¦éå¸¸æ‰å®çš„æ•°å­¦åŸºç¡€çš„ï¼Œå°±åƒResNeté‚£ä¹ˆç®€å•çš„ç»“æ„
4. è¿‡ç˜¾ï¼ğŸ‘ğŸ‘ğŸ‘

